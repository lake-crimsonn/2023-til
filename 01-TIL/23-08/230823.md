# 230823

# 수업

## 트랜스포머

- seq2seq는 언어를 번역할 때 사용한 모델. 컨텍스트 벡터를 이용한다. 앞선 문장을 압축하는데, 긴 문장이면 압축을 풀 때 손실이 생겨서 문제가 된다.
- attention은 seq2seq 문제를 해결함.
- rnn -> lstm -> seq2seq -> attention
- 어탠션은 핵심만 확인한다. 시퀀스와 과정은 비슷하다. 시퀀스는 모든 lstm의 컨텍스트를 합친 벡터를 사용했지만, 어탠션은 lstm 각각의 컨텍스트를 이용.
- 트랜스포머 논문의 부제 = 어탠션 이즈 올 유 니드
- 인코더를 어탠션으로 처리해서 디코더로 이용
- 트랜스포머 계열은 대용량의 데이터를 사용해야 한다.

## 비전 트랜스포머

- 포지셔널 임베딩이 포함되어서 사진을 그리드처럼 자른 후 순서를 기억하면서 인코딩을 함
- 요즘은 비전 트랜스포머를 cnn보다 많이 사용이 된다. 하지만 가지고 있는 데이터가 적다면 cnn이 더 낫다.

---
