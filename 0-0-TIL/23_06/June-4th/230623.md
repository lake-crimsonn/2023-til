# 230623
## 자습
>### 퍼셉트론
- 퍼셉트론은 MCP뉴런을 수학적으로 모델링했다. MCP뉴런은 신경세포을 간소화한 개념이다(맥컬린-피츠). 
- 뉴런은 여러 신경세포들의 신호를 가지돌기로 받고 합쳐서 단일 신호를 만든다. 신호의 세기가 특정한 임계치를 넘으면 다른 신경세포로 신호를 전달한다. 
- MCP뉴런이 신호를 출력할지 안할지 결정하는 과정을 모방하기 위해, 뉴런으로 들어오는 각 입력값에 가중치를 곱하고 자동으로 학습하는 알고리즘인 퍼셉트론이 제안되었다.
- 순입력함수는 x0 ~ xn 입력값에 각각 가중치 w0 ~ wn를 곱하고 합해서 하나의 값으로 만드는 함수다.
- 활성함수는 순입력함수의 리턴값이 임계치보다 높으면 1, 낮으면 -1를 출력하는 함수다.
- 퍼셉트론은 지도학습에 사용이 되는 알고리즘이다. 레이블이 되어 있는 실제 결과값이 있는 데이터를 가지고 학습을 한다. 활성함수를 통해 리턴된 값이 실제값과 다르면 두 값이 같아질 때까지 특정식에 의해 가중치 w0 ~ wn을 업데이트한다.
- wj = wj + n(y-y^)xj 퍼셉트론의 가중치를 업데이트하는 공식이다. 업데이트는 활성함수 리턴값과 실제 결과값이 같은지 다른지에 따라 이루어진다. 새로운 wj는 실제 결과값에 대한 활성함수 리턴값과 예측값에 대한 활성함수 리턴값의 차이에 학습률을 곱하고 피처값 xj 곱한 값을 이전 wj에 더한 값이다.
![perceptron](https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)
>### 아달라인
- 아달라인은 퍼셉트론과 활성함수가 다르다. 활성함수의 리턴값이 단순하게 실제값과 같은지 다른지를 비교하지 않고, 그들의 오차가 최소가 되도록하는 함수를 이용한다. 
>### 미분
- 미분 
  - 변수가 하나일 때
  - y=3 -> y`=0
  - y=2x+3 -> y`=2
  - y=x^2 -> y`=2x
- 편미분 partial derivative
  - 변수가 여러개 일 때
  - 특정 변수를 제외하고 상수로 간주
  - y=2a^2+3b+6
    - a에 대한 편미분
      - y=4a
    - b에 대한 편미분
      - y=3
- 합성함수 미분
  - f(x)=ax^2+b
  - g(x)=x^3
  - g(f(x)) = (ax^2+b)^3
  - = 3(ax^2+b)^2*(ax^2+b)` # 함성함수의 미분은 f(x)에 대한 미분도 해줘야 한다
  - = 3(ax^2+b)^2*2ax
>### 경사하강법
- 경사(기울기)를 구하여 경사가 낮은 쪽으로 계속 이동시켜서 0에 이를 때까지 반복해서 오차의 최소값을 찾는 방법.
- 비용함수(cost, loss function)를 최소화하기 위해 반복해서 파라미터를 업데이트하기 위해 사용.
- 러닝레이트는 w값들의 분포정도를 줄이기 위해서 사용한다
- [구현 코드](../../../3-deep-learning/230623-1-gd.ipynb)
>### 활성화함수
- 다층 퍼셉티콘 레이어를 이용하는 경우, 출력값이 다음 입력값으로 넘어갈 때 선형적으로 전파된다면 은닉층을 하나나 여러개나 차이가 없다. 이러한 이유로 출력층의 결과값을 비선형으로 변환시켜 줄 장치인 활성함수를 두게 되었다.
- 활성화함수는 다양하다. Sigmoid, Tanh, ReLU. 시그모이드나 탄은 기울기 소실문제가 생기기 떄문에 대부분의 인공신경망은 렐루를 사용한다.
>### 퍼셉트론 참고링크
- [링크 네이버](https://blog.naver.com/samsjang/220948258166)
- [링크 티스토리](https://needjarvis.tistory.com/181)